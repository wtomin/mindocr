English | [中文](README_CN.md)

# VisionLAN

<!--- Guideline: use url linked to abstract in ArXiv instead of PDF for fast loading.  -->

> VisionLAN: [From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network](https://arxiv.org/abs/2108.09661)

- [VisionLAN](#visionlan)
  - [1. Introduction](#1-introduction)
    - [1.1 VisionLAN](#11-visionlan)
  - [2. Results](#2-results)
  - [3. Quick Start](#3-quick-start)
    - [3.1 Installation](#31-installation)
    - [3.2 Dataset preparation](#32-dataset-preparation)
    - [3.3 Update yaml config file](#33-update-yaml-config-file)
    - [3.4 Training](#34-training)
    - [3.5 Evaluation](#35-evaluation)
    - [3.6 Predict](#36-predict)
  - [4. Inference](#4-inference)



## 1. Introduction

### 1.1 VisionLAN

 Visual Language Modeling Network (VisionLAN) is a text recognion model that learns the visual and linguistic information simultaneously via **character-wise occluded feature maps** in the training stage. This model does not require an extra language model to extract linguistic information, since the visual and linguistic information can be learned as a union.

 <image>

As shown above, the training pipeline of VisionLAN consists of three modules: (1) the backbone extract visual feature maps from the input image; (2) the Masked Language-aware Module (MLM) takes the visual feature maps and a randomly selected character index as inputs, and generates position-aware character mask map to create character-wise occluded feature maps; (3) finally, the Visual Reasonin Module (VRM) takes occluded feature maps as inputs and makes prediction under the complete word-level supervision.

While in the test stage, MLM is removed. Only the backbone and VRM are used for prediction.

## 2. Results

<results on six evaluation datasets>



## 3. Quick Start

### 3.1 Installation

Please refer to the [installation instruction](https://github.com/mindspore-lab/mindocr#installation) in MindOCR.

### 3.2 Dataset preparation

* Training sets

The authors of VisionLAN used two synthetic text datasets for training: SynthText(800k) and MJSynth. Please follow the instructions of the [original VisionLAN repository](https://github.com/wangyuxin87/VisionLAN) or download the two LMDB datasets from [BaiduYun](https://pan.baidu.com/s/1_2dqqxW1vDL9t3B-jlAYRw)(password:z0r5) or [Openi Platform](https://openi.pcl.ac.cn/ddeng/ocr_datasets_visionlan/datasets).

After download `SynthText.zip` and `MJSynth.zip`, please unzip and place them under `./datasets/train`.

* Evaluation sets

The authors of VisionLAN used six real text datasets for evaluation: IIIT5K Words (IIIT5K,) ICDAR 2013 (IC13), Street View Text (SVT), ICDAR 2015 (IC15), Street View Text-Perspective (SVTP), CUTE80 (CUTE). Please follow the instructions of the [original VisionLAN repository](https://github.com/wangyuxin87/VisionLAN) or download them from [BaiduYun](https://pan.baidu.com/s/1sUHgM982YiMf9kmtnhfirg) (password:fjyy) or [Openi Platform](https://openi.pcl.ac.cn/ddeng/ocr_datasets_visionlan/datasets).


After download `evaluation.zip`, please unzip and place them under `./datasets`.

The prepared dataset file struture should be:


``` text
datasets
├── evaluation
│   ├── Sumof6benchmarks
│   ├── CUTE
│   ├── IC13
│   ├── IC15
│   ├── IIIT5K
│   ├── SVT
│   └── SVTP
└── train
    ├── MJSynth
    └── SynText
```

### 3.3 Update yaml config file

If the datasets are placed under `./datasets`, there is no need to change the `train.dataset.dataset_root` in the yaml configuration file `configs/rec/visionlan/visionlan_L*.yaml`. Otherwise, change the following fields accordingly:

```yaml
...
train:
  dataset_sink_mode: False
  dataset:
    type: LMDBDataset
    dataset_root: dir/to/dataset          <--- Update
    data_dir: train                       <--- Update
...
eval:
  dataset_sink_mode: False
  dataset:
    type: LMDBDataset
    dataset_root: dir/to/dataset          <--- Update
    data_dir: evaluation                  <--- Update
...
```

> Optionally, change `num_workers` according to the cores of CPU, and change the `batch_size` according to the memory size.



### 3.4 Training

The training stages include Language-free (LF) and Language-aware (LA) process, and in total three steps for training:

```text
LF_1: train backbone and VRM, without training MLM
LF_2: train MLM and finetune backbone and VRM
LA: using the mask generated by MLM to occlude feature maps, train backbone, MLM, and VRM
```

Here we use distributed training for the three steps. For standalone training, please refer to the [recognition tutorial](../../../docs/en/tutorials/training_recognition_custom_dataset.md#model-training-and-evaluation).

```shell

mpirun --allow-run-as-root -n 8 python tools/train.py --config configs/rec/visionlan/visionlan_resnet45_LF_1.yaml

mpirun --allow-run-as-root -n 8 python tools/train.py --config configs/rec/visionlan/visionlan_resnet45_LF_2.yaml

mpirun --allow-run-as-root -n 8 python tools/train.py --config configs/rec/visionlan/visionlan_resnet45_LA.yaml
```

The training result (including checkpoints, per-epoch performance and curves) will be saved in the directory parsed by the arg `ckpt_save_dir` in yaml config file. The default directory is `./tmp_visionlan`.


### 3.5 Evaluation

After all three steps training, change the `system.distribute` to `False` in `configs/rec/visionlan/visionlan_resnet45_LA.yaml`, and then start evaluation on six evaluation sets seperately:


```shell


```


### 3.6 Predict

<predict on single image>

## 4. Inference
